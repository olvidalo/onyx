from __future__ import annotations

import datetime
import itertools
import tempfile
from collections.abc import Iterator
from typing import Any
from typing import cast
from typing import ClassVar

import pywikibot.time  # type: ignore[import-untyped]
from pywikibot import pagegenerators
from pywikibot import textlib

from onyx.configs.app_configs import INDEX_BATCH_SIZE
from onyx.configs.constants import DocumentSource
from onyx.connectors.interfaces import GenerateDocumentsOutput
from onyx.connectors.interfaces import LoadConnector
from onyx.connectors.interfaces import PollConnector
from onyx.connectors.interfaces import SecondsSinceUnixEpoch
from onyx.connectors.mediawiki.family import family_class_dispatch
from onyx.connectors.models import Document
from onyx.connectors.models import HierarchyNode
from onyx.connectors.models import ImageSection
from onyx.connectors.models import TextSection
from onyx.utils.logger import setup_logger


logger = setup_logger()

pywikibot.config.base_dir = tempfile.TemporaryDirectory().name


def safe_edittime_filter_generator(
    generator: Iterator[pywikibot.Page],
    last_edit_start: datetime.datetime | None = None,
    last_edit_end: datetime.datetime | None = None,
) -> Iterator[pywikibot.Page]:
    """Filter pages by edit time, skipping pages that cause errors.

    This is a safer replacement for pywikibot's EdittimeFilterPageGenerator
    which crashes on pages with problematic titles (e.g., containing colons
    that get misinterpreted as namespace separators).

    Args:
        generator: Page generator to filter.
        last_edit_start: Only include pages edited after this time.
        last_edit_end: Only include pages edited before this time.

    Yields:
        Pages that pass the time filter.
    """
    for page in generator:
        try:
            revision = page.latest_revision
            edit_time = pywikibot_timestamp_to_utc_datetime(revision.timestamp)

            if last_edit_start and edit_time < last_edit_start:
                continue
            if last_edit_end and edit_time > last_edit_end:
                continue

            yield page
        except Exception as e:
            logger.warning(f"Skipping page due to error checking edit time: {e}")
            continue


def pywikibot_timestamp_to_utc_datetime(
    timestamp: pywikibot.time.Timestamp,
) -> datetime.datetime:
    """Convert a pywikibot timestamp to a datetime object in UTC.

    Args:
        timestamp: The pywikibot timestamp to convert.

    Returns:
        A datetime object in UTC.
    """
    return datetime.datetime.astimezone(timestamp, tz=datetime.timezone.utc)


def get_doc_from_page(
    page: pywikibot.Page, site: pywikibot.Site | None, source_type: DocumentSource
) -> Document:
    """Generate Onyx Document from a MediaWiki page object.

    Args:
        page: Page from a MediaWiki site.
        site: MediaWiki site (used to parse the sections of the page using the site template, if available).
        source_type: Source of the document.

    Returns:
        Generated document.
    """
    page_text = page.text
    sections_extracted: textlib.Content = textlib.extract_sections(page_text, site)

    sections = [
        TextSection(
            link=f"{page.full_url()}#" + section.heading.replace(" ", "_"),
            text=section.title + section.content,
        )
        for section in sections_extracted.sections
    ]
    sections.append(
        TextSection(
            link=page.full_url(),
            text=sections_extracted.header,
        )
    )

    return Document(
        source=source_type,
        title=page.title(),
        doc_updated_at=pywikibot_timestamp_to_utc_datetime(
            page.latest_revision.timestamp
        ),
        sections=cast(list[TextSection | ImageSection], sections),
        semantic_identifier=page.title(),
        metadata={"categories": [category.title() for category in page.categories()]},
        id=f"MEDIAWIKI_{page.pageid}_{page.full_url()}",
    )


class MediaWikiConnector(LoadConnector, PollConnector):
    """A connector for MediaWiki wikis.

    Args:
        hostname: The hostname of the wiki.
        categories: The categories to include in the index.
        pages: The pages to include in the index.
        recurse_depth: The depth to recurse into categories. -1 means unbounded recursion.
        language_code: The language code of the wiki.
        batch_size: The batch size for loading documents.

    Raises:
        ValueError: If `recurse_depth` is not an integer greater than or equal to -1.
    """

    document_source_type: ClassVar[DocumentSource] = DocumentSource.MEDIAWIKI
    """DocumentSource type for all documents generated by instances of this class. Can be overridden for connectors
    tailored for specific sites."""

    def __init__(
        self,
        hostname: str,
        categories: list[str],
        pages: list[str],
        recurse_depth: int,
        language_code: str = "en",
        batch_size: int = INDEX_BATCH_SIZE,
    ) -> None:
        if recurse_depth < -1:
            raise ValueError(
                f"recurse_depth must be an integer greater than or equal to -1. Got {recurse_depth} instead."
            )
        # -1 means infinite recursion, which `pywikibot` will only do with `True`
        self.recurse_depth: bool | int = True if recurse_depth == -1 else recurse_depth

        self.batch_size = batch_size
        self.hostname = hostname
        self.language_code = language_code
        self._category_names = categories
        self._page_names = pages

        # Defer site creation until credentials are loaded (needed for private wikis)
        self.family: Any = None
        self.site: Any = None
        self.categories: list[Any] = []
        self.pages: list[Any] = []
        self._credentials: dict[str, Any] = {}

    def _initialize_site(self) -> None:
        """Initialize the pywikibot site connection. Called after credentials are loaded."""
        if self.site is not None:
            return

        # For private wikis, configure pywikibot with credentials BEFORE creating Site
        username = self._credentials.get("mediawiki_username")
        password = self._credentials.get("mediawiki_password")

        if username and password:
            # Configure pywikibot to use these credentials
            # This sets up the user-config so pywikibot can authenticate
            family_name = "WikipediaConnector"
            pywikibot.config.usernames[family_name] = {self.language_code: username}
            pywikibot.config.password_file = None  # Don't use password file
            # Store password in pywikibot's password list
            pywikibot.config.authenticate[self.hostname] = (username, password)
            logger.info(f"Configured pywikibot credentials for user: {username}")

        # short names can only have ascii letters and digits
        self.family = family_class_dispatch(
            self.hostname, "WikipediaConnector", self.language_code
        )()
        self.site = pywikibot.Site(fam=self.family, code=self.language_code, user=username if username else None)

        # Login if credentials provided
        if username and password:
            logger.info(f"Attempting to log in to MediaWiki site as user: {username}")
            try:
                from pywikibot.login import ClientLoginManager
                login_manager = ClientLoginManager(password=password, site=self.site, user=username)
                result = login_manager.login()
                logger.info(f"Login result: {result}")
                if not result:
                    raise RuntimeError("Login failed - check username and password")
                logger.info(f"Successfully logged in to MediaWiki site as: {username}")
            except Exception as e:
                import traceback
                logger.error(f"Failed to log in to MediaWiki site: {type(e).__name__}: {e}")
                logger.error(f"Traceback: {traceback.format_exc()}")
                raise RuntimeError(f"MediaWiki authentication failed: {type(e).__name__}: {e}") from e

        self.categories = [
            pywikibot.Category(
                self.site,
                (
                    f"{category.replace(' ', '_')}"
                    if category.startswith("Category:")
                    else f"Category:{category.replace(' ', '_')}"
                ),
            )
            for category in self._category_names
        ]

        self.pages = []
        for page in self._page_names:
            if not page:
                continue
            self.pages.append(pywikibot.Page(self.site, page))

    def load_credentials(self, credentials: dict[str, Any]) -> dict[str, Any] | None:
        """Load credentials for a MediaWiki site.

        Supports authentication via username and password (or bot password).
        If credentials are provided, the connector will attempt to log in to the site.

        Args:
            credentials: Dictionary containing optional 'mediawiki_username' and
                        'mediawiki_password' keys.

        Note:
            For most read-only operations, MediaWiki API credentials are not necessary.
            However, some wikis require authentication to access content.
        """
        logger.info(f"MediaWiki load_credentials called with keys: {list(credentials.keys())}")
        username = credentials.get("mediawiki_username")
        password = credentials.get("mediawiki_password")
        logger.info(f"MediaWiki credentials - username present: {bool(username)}, password present: {bool(password)}")

        # Store credentials for use during site initialization
        self._credentials = credentials

        # Initialize site connection with credentials
        self._initialize_site()

        if not username or not password:
            logger.warning("MediaWiki credentials not provided or incomplete - skipping login")

        return None

    def _get_doc_batch(
        self,
        start: SecondsSinceUnixEpoch | None = None,
        end: SecondsSinceUnixEpoch | None = None,
    ) -> GenerateDocumentsOutput:
        """Request batches of pages from a MediaWiki site.

        Args:
            start: The beginning of the time period of pages to request.
            end: The end of the time period of pages to request.

        Yields:
            Lists of Documents containing each parsed page in a batch.
        """
        # Ensure site is initialized (for public wikis without credentials)
        self._initialize_site()

        doc_batch: list[Document | HierarchyNode] = []

        # Pywikibot can handle batching for us, including only loading page contents when we finally request them.
        if self.categories:
            # Fetch pages from specified categories
            category_pages = []
            for category in self.categories:
                cat_gen = pagegenerators.CategorizedPageGenerator(
                    category, recurse=self.recurse_depth
                )
                # Only apply time filter if start or end is specified
                if start or end:
                    cat_gen = safe_edittime_filter_generator(
                        cat_gen,
                        last_edit_start=(
                            datetime.datetime.fromtimestamp(start, tz=datetime.timezone.utc) if start else None
                        ),
                        last_edit_end=datetime.datetime.fromtimestamp(end, tz=datetime.timezone.utc) if end else None,
                    )
                category_pages.append(
                    pagegenerators.PreloadingGenerator(cat_gen, groupsize=self.batch_size)
                )
            all_pages: Iterator[pywikibot.Page] = itertools.chain(
                self.pages, *category_pages
            )
        else:
            # No categories specified - fetch ALL pages from the wiki
            logger.info("No categories specified, fetching all pages from wiki")
            base_generator = pagegenerators.AllpagesPageGenerator(
                site=self.site,
                namespace=0,  # Main namespace only (articles)
            )
            # Only apply time filter if start or end is specified
            if start or end:
                base_generator = safe_edittime_filter_generator(
                    base_generator,
                    last_edit_start=(
                        datetime.datetime.fromtimestamp(start, tz=datetime.timezone.utc) if start else None
                    ),
                    last_edit_end=datetime.datetime.fromtimestamp(end, tz=datetime.timezone.utc) if end else None,
                )
            all_pages_generator = pagegenerators.PreloadingGenerator(
                base_generator,
                groupsize=self.batch_size,
            )
            all_pages = itertools.chain(self.pages, all_pages_generator)
        for page in all_pages:
            try:
                # Skip pages that don't exist or are interwiki links
                if not page.exists():
                    logger.debug(f"Skipping non-existent page: {page.title()}")
                    continue
                logger.info(
                    f"MediaWikiConnector: title='{page.title()}' url={page.full_url()}"
                )
                doc_batch.append(
                    get_doc_from_page(page, self.site, self.document_source_type)
                )
            except Exception as e:
                logger.warning(f"Skipping page due to error: {e}")
                continue
            if len(doc_batch) >= self.batch_size:
                yield doc_batch
                doc_batch = []
        if doc_batch:
            yield doc_batch

    def load_from_state(self) -> GenerateDocumentsOutput:
        """Load all documents from the source.

        Returns:
            A generator of documents.
        """
        return self.poll_source(None, None)

    def poll_source(
        self, start: SecondsSinceUnixEpoch | None, end: SecondsSinceUnixEpoch | None
    ) -> GenerateDocumentsOutput:
        """Poll the source for new documents.

        Args:
            start: The start of the time range to poll.
            end: The end of the time range to poll.

        Returns:
            A generator of documents.
        """
        return self._get_doc_batch(start, end)


if __name__ == "__main__":
    HOSTNAME = "fallout.fandom.com"
    test_connector = MediaWikiConnector(
        hostname=HOSTNAME,
        categories=["Fallout:_New_Vegas_factions"],
        pages=["Fallout: New Vegas"],
        recurse_depth=1,
    )

    all_docs = list(test_connector.load_from_state())
    print("All docs", all_docs)
    current = datetime.datetime.now().timestamp()
    one_day_ago = current - 30 * 24 * 60 * 60  # 30 days

    latest_docs = list(test_connector.poll_source(one_day_ago, current))

    print("Latest docs", latest_docs)
